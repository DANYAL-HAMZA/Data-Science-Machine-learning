{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbe89ac-974c-46ed-b2e9-878090093b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c319bc0-b5de-4f6c-8f22-5f53f41c04cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/',download=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811a02b0-89d7-47d9-ad20-c4c481e8b3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb007ea-d073-4bb8-b770-67b2328427f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_tensor,label = dataset[0]\n",
    "plt.imshow(image_tensor[0,0:25,0:25],cmap='gray')\n",
    "print('label:',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d8253f-208b-4c41-ae91-d36b5f68e889",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_indices(n,val_pct):\n",
    "    #determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "    #create a random permutation of 0 to n-1\n",
    "    idx = np.random.permutation(n)\n",
    "    #pick first n_val indices for validation set\n",
    "    return idx[n_val:],idx[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589dd548-c480-4eb9-b5e2-ea4cefe72144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_indices,val_indices = split_indices(len(dataset), val_pct=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3d0912-f839-4a28-bcea-e77c570d1815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n",
      "sample val_indices: [50845 54788 11316 48791 26382 38903 43567 33525 33434  9314  3445 17879\n",
      " 53695 34267 46021 53414 53392 57537 10332 55283]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices),len(val_indices))\n",
    "print('sample val_indices:',val_indices[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc2a249-ec5f-40f1-aa8c-812aa3fa3200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "#define train_sampler and train_loader\n",
    "\n",
    "train_sampler =SubsetRandomSampler(train_indices)\n",
    "train_loader = DataLoader(dataset,batch_size,sampler=train_sampler)\n",
    "\n",
    "#validation_sampler and validation_loader\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset,batch_size,sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874cdb1d-f1a1-4c19-b68f-415f0a41e655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "#logistic regression model\n",
    "\n",
    "model = nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0029a4-ee06-4459-8e98-771f7efc7c83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_loader:\n",
    "    print(xb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3d9e5b-04b4-4ca4-bbc6-072d3ed8704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using an image size of 100 by 84 so we use the reshape or view method to convert as follow\n",
    "#xb = xb.reshape(100,784), where 100 is the batch number,but we cannot change the batch size hence we do the following instead\n",
    "#xb = xb.view(xb.size(0)),784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96943f6-3621-42ae-bb75-158ff6307761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to improve upon the logistic  regression,we will create a neural network with one hidden layer\n",
    "#--instead of using a single nn.linear object to transform a batch of inputs(pixel intensities) into a batch of outputs(class\n",
    "#probabilities),we will use two nn.linear objects.Each is called a layer in the neural network.\n",
    "#--the first layer,called the hidden layer will transform the input matrix of shape (batch_size x 784) \n",
    "#into an intermediate output matrix of shape (batch_size x hidden_size),\n",
    "#where hidden size is a preconfigured parameter(eg,32 or 64)\n",
    "#--the intermediate outputs are then passed into a non linear activation function,which operates on individual\n",
    "#elements of the output matrix\n",
    "#--the result of the activation function,which is also of size (batch_size x hidden_size),is passed into the second  layer\n",
    "#also known as the output layer,which transforms it into a matrix of size(batch_size x 10),\n",
    "#identical to the output of the logistic regression\n",
    "#the activation function we will use here is called the ReLU(Rectified Linear Unit) with formula\n",
    "#relu(x) = max(0,x).That is if an element is negative,we replace it by 0,otherwise we leave it unchanged.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f7e707-1824-4054-93d1-7bedced92378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    #feedfoward neural network with one hidden layer\n",
    "    def __init__(self,in_size,hidden_size,out_size):\n",
    "        super().__init__()\n",
    "        #hidden layer\n",
    "        self.linear = nn.Linear(in_size,hidden_size)\n",
    "        #output layer\n",
    "        self.linear2 = nn.Linear(hidden_size,out_size)\n",
    "    def forward(self,xb):\n",
    "            #flatten the image tensor using the view(ensure that we are using the same block of memory) or reshape method\n",
    "            xb = xb.view(xb.size(0),-1)\n",
    "            #from above,784(28x28) is the image size we must use but it is only so in this case but\n",
    "            #might differ with different images so we use (-1) to allow pytorch to calculate it for us \n",
    "            #get intermediate output using hidden layer\n",
    "            out = self.linear(xb)\n",
    "            #apply activation function\n",
    "            out = f.relu(out)\n",
    "            #get predictions using output layer\n",
    "            out = self.linear2(out)\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e2cc1c0-8ffa-4b4f-b0e7-cc0d0ecacbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec54ff3-e513-44cc-8790-99b0fea665f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will create a model with a hidden layer of 32 activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53280b37-a302-41f4-8f83-4076d2ffe32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MnistModel(input_size,hidden_size=32,out_size=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783ccd31-e62f-45d2-81e2-06df5f37f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take a look at the models parameters,we  expect to see one weight and bias matrix for each of the network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef96d57-e120-4868-a997-f7c274b540ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for t in model.parameters():\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2272b3f1-6944-4200-bba4-3ee4ee73a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try and generate some outputs using our model, we will take the first batch of 100 images from \n",
    "#our dataset,and pass them into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f663921d-213b-47c4-87f0-ca83d17fe3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3229575157165527\n",
      "output.shape: torch.Size([100, 10])\n",
      "sample outputs: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    loss = f.cross_entropy(outputs,labels)\n",
    "    print('loss:',loss.item())\n",
    "    print('output.shape:',outputs.shape)\n",
    "    print('sample outputs:',outputs[:2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd738f41-1332-40d4-b62e-adedf3ebaf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = f.softmax(outputs,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545ec911-7023-4813-aef2-a11d990c4082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_probs,preds = torch.max(probs,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3aa0fd-9bb6-4691-8a00-4d98c746e6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 3, 5, 8, 8, 5, 5, 2, 2, 5, 8, 5, 3, 5, 2, 3, 2, 3, 3, 5, 1, 5, 2,\n",
      "        2, 3, 3, 3, 3, 5, 5, 3, 5, 5, 5, 5, 8, 5, 3, 3, 3, 2, 8, 5, 3, 2, 3, 2,\n",
      "        5, 8, 5, 5, 6, 5, 3, 3, 2, 5, 5, 2, 5, 5, 3, 5, 3, 6, 5, 2, 5, 5, 2, 2,\n",
      "        5, 5, 5, 5, 5, 3, 5, 5, 3, 3, 5, 3, 3, 3, 8, 5, 5, 3, 3, 3, 5, 2, 5, 2,\n",
      "        8, 2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4397e88a-2a1b-4293-9c2c-e50313bbc243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False,  True, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,  True,\n",
       "        False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels==preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c077f3f-27a3-4441-8c3a-5c9873b58dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING GPU's \n",
    "\n",
    "#as the size of our model and datasets increase, we need to use GPU's to train our models within a reasonable amount of time\n",
    "#GPU's contain hundreds of cores that are optimized for performing expensive matrix operations on floating\n",
    "#point numbers in a short period of time,which makes them ideal for  training deep nueral networks with many layers.\n",
    "#you can use GPU's for free on kaggle kernels or googgle colab,or rent GPU powered machines \n",
    "#on services like Google Cloud Platforms,Amazon Web services or paperspace\n",
    "#we can check if GPU is available and also if the required NVIDIA CUDA + drivers are available usong (torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4629b94-dfb2-4b97-9cbb-e02510d8758e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868064aa-557a-44b3-9e18-b3dd2fbaf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define a helper function to ensure that our code uses the GPU if available and defaults to using the CPU if it isnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed5aabd6-e66a-496a-80b4-6fd23b3221fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340f78e7-a16a-44ff-801e-9de808203a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed2e839-5a9d-480e-952e-c8ba4847d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define a method that can move data and model to a chosen device\n",
    "#we send data or models to GPU's, the GPU trains the model in a shortest time and returns the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c71e9e1d-8299-4920-9e5f-6a6a1ef8f417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    #move tensor to chosen device\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29925bc7-97f2-4aa4-ae8a-5ed49ac2883b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\n",
    "    print(images.shape)\n",
    "    images = to_device(images,device)\n",
    "    print(images.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfafcb3e-955b-40e2-95cb-49fb4216e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally we define a DeviceDataLoader class to wrap loaders and move move data to the selected device\n",
    "#as the batches are accessed.We dont need to extend an existing class to to create a pytorch pytorch dataloader.\n",
    "#All we need is an __iter__() function to retrieve batches of data,a __len__ method to get the number of batches\n",
    "#data is moved to the device at the exact point when the data is being passed ito the model to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f2aea96-909d-4594-8310-c4bcdfe2b3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeviceLoader():\n",
    "    #wrap a dataloader to move data to a device\n",
    "    def __init__(self,dataloader,device):\n",
    "        self.dataloader=dataloader\n",
    "        self.device=device\n",
    "    def __iter__(self):\n",
    "        #yield a batch of data after moving it to device\n",
    "        for b in self.dataloader:\n",
    "            #yield pauses the execution of an instance of the function and executes the function on the next call\n",
    "            yield to_device(b,self.device)\n",
    "    def __len__(self):\n",
    "        #return number of batches\n",
    "        return len(self.dataloader)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00206c46-7f35-4a94-877f-a8578a7394e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now wrap our dataloaders using our DeviceDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5270f5bf-6dd7-4e11-85c9-0bade54a3a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DeviceLoader(train_loader,device)\n",
    "val_loader = DeviceLoader(val_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3d64e53-9eb1-44f5-bf7f-6744c94262cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yb: tensor([6, 0, 3, 0, 1, 1, 9, 6, 1, 4, 0, 1, 6, 3, 1, 7, 3, 7, 6, 1, 0, 0, 7, 6,\n",
      "        7, 8, 9, 6, 1, 6, 2, 4, 3, 9, 8, 6, 4, 5, 7, 2, 9, 9, 1, 8, 9, 5, 7, 1,\n",
      "        9, 5, 0, 1, 6, 8, 9, 0, 3, 6, 3, 9, 0, 3, 2, 9, 8, 1, 9, 7, 7, 7, 5, 9,\n",
      "        8, 3, 4, 7, 2, 8, 8, 5, 8, 8, 4, 6, 6, 8, 7, 7, 6, 0, 7, 2, 1, 1, 6, 6,\n",
      "        8, 8, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_loader:\n",
    "    print('yb:',yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4fc8b58-d684-4fe1-a39a-a931fdbe1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52716abe-fd08-453e-bb6c-7c850c67f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for given num of epochs\n",
    "def loss_batch(model,loss_fn,xb,yb,opt=None,metric=None):\n",
    "\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds,yb)\n",
    "        if opt is not None:\n",
    "            #compute gradient of loss\n",
    "            loss.backward()\n",
    "            #update predictions using gradients\n",
    "            opt.step()\n",
    "            #reset gradients to zero\n",
    "            opt.zero_grad()\n",
    "            metric_result = None\n",
    "        if metric is not None:\n",
    "                #compute metric\n",
    "                metric_result = metric(preds,yb)\n",
    "                \n",
    "        return loss.item(),len(xb),metric_result\n",
    "                \n",
    "            \n",
    "    \n",
    "            \n",
    "                  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99602bf-349b-4ff7-9351-f5a89fa9f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn,valid_dl,metric=None):\n",
    "    with torch.no_grad():\n",
    "        #pass each batch through the model\n",
    "        results = [loss_batch(model,loss_fn,xb,yb,metric=metric) for xb,yb in valid_dl]\n",
    "        #separate losses,counts and metrics\n",
    "        losses,nums,metrics = zip(*results)\n",
    "        #total size of the dataset\n",
    "        total = np.sum(nums)\n",
    "        #average loss across batches\n",
    "        avg_loss = np.sum(np.multiply(losses,nums))/total\n",
    "        avg_metric = None\n",
    "    if metric is not None:\n",
    "        #average of metric across batches\n",
    "        avg_metric = np.sum(np.multiply(metrics,nums))/total\n",
    "    return avg_loss,total,avg_metric\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3746ff-da11-4897-8eb7-d7be8f2959ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd8e4dc5-2862-4d76-9709-35e755f2ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,loss_fn,train_dl,valid_dl,metric=None,opt_fn=None):\n",
    "    loss,metrics = [],[]\n",
    "    #instantiate the optimizer\n",
    "    if opt_fn is None:\n",
    "        opt_fn = torch.optim.SGD\n",
    "        opt=opt_fn(model.parameters(),lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        #training of training dataset\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model,loss_fn,xb,yb,opt,metric=accuracy)\n",
    "            #evaluation of evaluation dataset\n",
    "            result = evaluate(model,loss_fn,valid_dl,metric)\n",
    "            avg_loss,total,avg_metric = result\n",
    "            #record the loss and metrics\n",
    "            losses.append(val_loss)\n",
    "            metrics.append(val_metric)\n",
    "            #print progress\n",
    "    if metric is None:\n",
    "        print('Epoch[{}/{}],Loss:{:.4f}'.format(epoch+1,epochs,val_loss))\n",
    "    else:\n",
    "            print('Epoch[{}/{}],Loss:{:.4f},{}:{:.4f}'.format(epoch+1,epochs,val_loss,metric.__name__,val_loss))\n",
    "            \n",
    "        \n",
    "              \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b3ca85f-7015-4e21-8914-29fd68a0254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    max_probs,preds = torch.max(outputs,dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32cd818a-89ff-4d6f-8d05-4f173e8fb42b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (linear): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MnistModel(input_size,hidden_size=32,out_size=num_classes)\n",
    "#we use the to_device method to move the model's parameters to thhe right device\n",
    "to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78774f3b-a379-4a3a-bbb2-a757a39c26f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.3182,accuracy:0.1179\n"
     ]
    }
   ],
   "source": [
    "#lets see how the model performs on validation set with the initial set of weight and biases\n",
    "val_loss,total,val_acc = evaluate(model,f.cross_entropy,val_loader,metric=accuracy)\n",
    "print('loss:{:.4f},accuracy:{:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ba70d-70d8-4dab-af27-3bf019cba8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the accuracy is around 10% which is what one might expect from a randomly initialized model(since it has a 1 in 10 chances \n",
    "#of getting a label right by guessing randomly)\n",
    "#we now train the model for 5 epoch and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072bb86-03b5-412f-b316-d2b805c34046",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1,metrics1 = fit(5,0.5,model,f.cross_entropy,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33a1be-95ec-41f6-8254-6f8b3111f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from te result above,we have already reached 96% accuracy which is way better than what we had in \n",
    "#our logistic regression model.To increse the accuracy further, we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e8f5d-e75b-48ec-b8c1-e48bf63337d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
