{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365639a-66d0-4722-9dcf-947ed975df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATIVE ADVERSERIAL NETWORK\n",
    "\n",
    "#-DEFINE PROBLEM STATEMENT\n",
    "#-load the data(with transforms and normalization)\n",
    "   #-denormalize for visual inspection and of samples\n",
    "    \n",
    "#-DEFINE DISCRIMINATOR NETWORK\n",
    "   #-study the activation functions:leaky relu\n",
    "    \n",
    "#-DEFINE GENERATOR NETWORK\n",
    "   #-explain the output generator function \n",
    "    #-look at some sample output\n",
    "    \n",
    "#-DEFINE LOSSES,OPTIMIZERS AND HELPER FUNCTIONS FOR TRAINING\n",
    "   #-for discriminator\n",
    "    #-for generator\n",
    "    \n",
    "#-TRAIN THE MODEL\n",
    "   #-save intermediate generated images to file\n",
    "    \n",
    "#LOOK AT SOME OUTPUTS\n",
    "\n",
    "#-SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608ce3a-8723-4b49-b49e-2f214fde1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we begin by downloading and importing the data as a pytorch adataset using the MNISThelper class from torchvision.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f3294f-8afa-455b-a624-a6f7260cc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor,Normalize,Compose\n",
    "from torchvision.datasets import MNIST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2878420b-b2db-41c6-9673-677f345dc416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mnist \u001b[38;5;241m=\u001b[39m MNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mCompose([ToTensor(),Normalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m,),\n\u001b[0;32m      2\u001b[0m                                                                                             std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m))]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:102\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "mnist = MNIST(root='data',train=True,download=True,transform=Compose([ToTensor(),Normalize(mean=(0.5,),\n",
    "                                                                                            std=(0.5))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98434e-066f-48b2-bd08-b7ca75566a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that we are transfering the pixel values from the range [0,1] to [-1,1].The reason for doing this will become clear when\n",
    "#we define the generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1cb0a-3398-48f1-b63b-954eff3135f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = mnist[0]\n",
    "print(label)\n",
    "print(img[:,10:15,10:15])\n",
    "torch.min(img),torch.max(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f717815-f726-49e2-8f10-313b18e3a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as expected, the pixel values range from -1 to 1. lets define a helper function to denormalize and view the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b303f81-b204-4046-935a-42af9217e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out=(x+x)/2\n",
    "    return out.clamp(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0613-0018-4c53-9a42-9a89a8eb7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img_norm = denorm(img)\n",
    "plt.imshow(img_norm[0],cmap='gray')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780b4f1-ebbf-4eea-869e-69e654652f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=100\n",
    "data_loader = DataLoader(mnist,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350e60e-fbcf-4df0-ba6c-10314e6ca8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_batch,label_batch in data_loader:\n",
    "    print('first batch')\n",
    "    print(img_batch.shape)\n",
    "    plt.imshow(img_batch[0][0],cmap='gray')\n",
    "    print(label_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5e3d-462d-46b7-b6fe-6d2e900785e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda',if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24125d54-9a52-456a-9692-eae249e6df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCRIMINATOR NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf0338-cf0b-4e23-9439-1bdb555c1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "image_size = 784\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd3fc6-3876-44cf-923c-1238de9f0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = nn.Sequential(nn.Linear(image_size,hidden_size),\n",
    "                  nn.LeakyRelu(0.2)\n",
    "                  nn.Linaer(hidden_size,hidden_size)\n",
    "                  nn.LeakyRelu(0.2)\n",
    "                  nn.Linear(hidden_size,1)\n",
    "                  nn.Sigmoid())\n",
    "                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8bdc3-2fdc-45a5-8eb5-ad44e2ee2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2764-ac38-45e9-85c2-13a7ad3c3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the leakyrelu activation function we multiply any value less(negative) than zero\n",
    "#by the LeakyRelu parameter, whereas in the Relu activation function, we put any negative value to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d381647-5841-4049-8442-dd2c911872aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATOR NETWORK/MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b819d1-f432-432b-a68f-24e5f851f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size=64\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size,hidden_size),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(hidden_size,hidden_size),\n",
    "    nn.Relu(),\n",
    "    nn.Linear(hidden_size,image_size)\n",
    "    #we use the Tanh activation function for the output of the generator\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faea4b7-05b3-45bf-84e3-e3cbafa335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = G(torch.randn(2,latent_size))\n",
    "# convrert a two image vector to a 28x28 pixels\n",
    "y.reshape(-1,28,28).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd50c1-ec2c-4f8c-81cd-e60fa01de184",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img=denorm(y.reshape((-1,28,28).detach())\n",
    "               #detach return the neutral value without gradients etc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b500c-6210-4aae-a0bf-4113afb28eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gen_img[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809ae08-6b4a-49b7-b2f3-d6337b6dfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as expected the image is very noisy because we have no trained it yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d72169-e8e5-4cf1-b4cb-7aa71732f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCRIMINATOR MODEL\n",
    "\n",
    "#since the discriminator is a binary classification model,we can use the binary cross entropy loss function from the nn.BCELoss\n",
    "#to quantify how well it is able to differentiate between real and generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732073d-600f-4a82-8974-47ebb0dbe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(),lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(),lr=0.0002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee14ecb-8cad-451c-b5bb-f2bc1d7fe6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define a helper function to reset gradients and train the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18988cc5-77d1-4484-a2b8-6042e25dda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting gradients back to zero can be done at the beginning of training\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "def train_discriminator(images):\n",
    "    #create the labels which are later used as inputs for the BCELoss\n",
    "    real_labels=torch.ones(batch_size,1).to(device)\n",
    "    fake_labels=torch.zeros(batch_size,1).to(device)\n",
    "    \n",
    "    #loss for real images\n",
    "    output = D(images)\n",
    "    d_loss_real=criterion(outputs,real_labels)\n",
    "    real_score = outputs\n",
    "    \n",
    "    #loss for fake images\n",
    "    #we generate 100 latent vectors to feed into the generator model and move them to the device\n",
    "    #this is because we are not using the MNIST dataset here but rather generating random images ourselves \n",
    "    z = torch.randn(batch_size,latent_size).to(device)\n",
    "    fake_images = G(z)\n",
    "    outputs = D(fake_images)\n",
    "    d_loss_fake = criterion(outputs,fake_labels)\n",
    "    fake_score = outputs\n",
    "    \n",
    "    #combine losses\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    #reset gradient\n",
    "    reset_grad()\n",
    "    #compute gradient for loss\n",
    "    d_loss.backward()\n",
    "    #adjust the parameters using backprop\n",
    "    d_optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8b25c-07a7-45ae-a0fa-24e42b197386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATOR TRAINING\n",
    "\n",
    "#since the outputs of the generator are vectors(which can be transformed to images),it is not obvious how we\n",
    "#can train the generator.Since we know that the output images are fake,we can pass them into the discriminator,and compare the \n",
    "#output of the discriminator with the ground truth(ie. all fake) and use that to calculate the loss for the generator. In other\n",
    "#words we can use the discriminator itself as part of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d63568-8e21-41e3-8483-338100e33873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "   # generate the fake images and calculate loss\n",
    "z = torch.randn(batch_size,latent_size,1).to(device)\n",
    "fake images = G(z)\n",
    "outputs = D(fake_images)\n",
    "#the generator should fool the discriminator to think that the images the generator produces is real.\n",
    "#we therefore used torch.ones in this scenario,although the output images are fake \n",
    "#and should have an output of zero.This is so that the discriminator outputs will be closer to 1,thinking that the \n",
    "#fake images are actually real\n",
    "labels = torch.ones(batch_size,1).to(device)\n",
    "g_loss = criterion(outputs,labels)\n",
    "#backprop and optimize\n",
    "reset_grad()\n",
    "g_loss.backward()\n",
    "g_optimizer.step()\n",
    "return g_loss,fake_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8844aa4-0465-474a-b5e8-976647caac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE MODEL\n",
    "\n",
    "#lets create a dictionary to where we can save intermediate outputs from the output to visually\n",
    "#inspect the progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74915664-3dc0-40c5-9762-7c5b31211c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir)\n",
    "os.makedir(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e427d2-962b-4beb-9044-72ee7c7df48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets save a batch or real images that we can use for visual comparism while looking at the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584ba7b-e276-4557-af87-de3be697e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPYTHON.display import image\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd839f4-6f8b-4d95-b89a-26e2d5b89283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save some real images\n",
    "for images,_ in data_loader:\n",
    "    images = images.reshape(images.size(0),1,28,28)\n",
    "    save_image(denorm(images),os.path.join(sample_dir,'real_images.png'),nrow=10)\n",
    "    #the nrow tarameter is the number of images in each row of a grid data.It allows you to create a grid of images\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e41c1d-c7e3-4a96-89c2-cb68eb27ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the image class from IPYTHON is used to view the image\n",
    "image(os.path.join(sample_dir,'real_image.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac0834-d08a-4187-b7b8-963182b2f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now define a helper function to save a batch of generated images to disk at the end of every epoch\n",
    "#we will use a fixed set of input vectors to the generator to see how the individual generated images \n",
    "#evolve over time as we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70049d36-c393-4b54-9048-71aea0dbf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vectors = torch.randn(batch_size,latent_size).to(device)\n",
    "\n",
    "def save_fake_images(index):\n",
    "    fake_images = G(sample_vectors)\n",
    "    fake_images =fake_images.reshape(fake_images.size(0),1,28,28)\n",
    "    fake_fname='fake_images-(0:0-4d).png'.format(index)\n",
    "    print('saving',fake_fname)\n",
    "    save_image(denorm(fake_images),os.path.join(sample_dir,fake_fname),nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe5f2b-b0d2-4ef0-8912-31d89d29d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before training\n",
    "#since multiple fake images are produced,the index zero(0) here indicates the first image\n",
    "save_fake_images(0)\n",
    "image(os.path.join(sample_dir,'fake_images-0000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72b710-6849-472f-8673-130a13f7a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as expected, the images are very noisy\n",
    "#we are now ready to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f203326-85a6-4e87-a690-33f238666a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "total_step=len(data_loader)\n",
    "d_losses,g_losses,real_scores,fake_scores=[],[],[],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,_) in enumerate(data_loader):\n",
    "        #load a batch and transform to vectors\n",
    "        images = images.reshape(batch_size,-1).to(device)\n",
    "        #train discriminator and generator\n",
    "        d_loss,real_score,fake_score=train_discriminator(images)\n",
    "        g_loss,fake_images=train_generator()\n",
    "        #inspect the losses\n",
    "        if (i+1)%200=0:\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            real_scores.append(real_score.mean().item())\n",
    "            fake_scores.append(fake_score.mean().item())\n",
    "            print('epoch [{}/{}], step [{}/{}], d_losses:{:.4f}, g_losses:{:.4f}, D(x):{:.4f},\n",
    "                  .format(epoch,num_epochs,i+1,total_step,d_loss.item(),g_loss.item(),real_score.mean().item()),\n",
    "                  fake_score.mean().item()))\n",
    "    #sample and save images\n",
    "    save_fake_images(epoch+1)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43377d2-c048-482e-a95d-d08f06ef9f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
